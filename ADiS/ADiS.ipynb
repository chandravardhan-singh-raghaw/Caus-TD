{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svtSpMtTw5D4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from einops import rearrange\n",
        "\n",
        "from diffusers import DDPMScheduler\n",
        "from diffusers.models.attention import BasicTransformerBlock\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from accelerate import Accelerator\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "high_res_image_size = 64\n",
        "low_res_image_size = 32\n",
        "channels = 3\n",
        "\n",
        "num_train_timesteps = 1000\n",
        "num_inference_steps = 50\n",
        "learning_rate = 2e-4\n",
        "epochs = 100\n",
        "num_cra_blocks = 3\n",
        "encoder_scale_factor_r = 8\n",
        "\n",
        "save_image_epochs = 2\n",
        "eval_batch_size = 16\n",
        "gradient_accumulation_steps = 1\n",
        "mixed_precision = \"fp16\"\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "num_workers = 4\n",
        "prefetch_factor = 2\n",
        "\n",
        "output_dir = \"./adis_model_output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/samples\", exist_ok=True)\n",
        "\n",
        "tensorboard_dir = f\"{output_dir}/tensorboard\"\n",
        "os.makedirs(tensorboard_dir, exist_ok=True)\n",
        "writer = SummaryWriter(tensorboard_dir)\n",
        "\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, high_res_transform, low_res_transform):\n",
        "        self.root_dir = root_dir\n",
        "        self.high_res_transform = high_res_transform\n",
        "        self.low_res_transform = low_res_transform\n",
        "        self.image_paths = []\n",
        "\n",
        "        for subdir in [\"infected\", \"notinfected\"]:\n",
        "            subdir_path = os.path.join(root_dir, subdir)\n",
        "            if os.path.exists(subdir_path):\n",
        "                for file in os.listdir(subdir_path):\n",
        "                    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp')):\n",
        "                        self.image_paths.append(os.path.join(subdir_path, file))\n",
        "\n",
        "        print(f\"Found {len(self.image_paths)} images in dataset\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            high_res_image = self.high_res_transform(image)\n",
        "            low_res_image = self.low_res_transform(image)\n",
        "            return (high_res_image, low_res_image), 0\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            placeholder_high = torch.zeros((channels, high_res_image_size, high_res_image_size))\n",
        "            placeholder_low = torch.zeros((channels, low_res_image_size, low_res_image_size))\n",
        "            return (placeholder_high, placeholder_low), 0\n",
        "\n",
        "high_res_transform = transforms.Compose([\n",
        "    transforms.Resize((high_res_image_size, high_res_image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5] * channels, [0.5] * channels)\n",
        "])\n",
        "\n",
        "low_res_transform = transforms.Compose([\n",
        "    transforms.Resize((low_res_image_size, low_res_image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5] * channels, [0.5] * channels)\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = CustomImageDataset(\n",
        "    root_dir=\"./data/train\",\n",
        "    high_res_transform=high_res_transform,\n",
        "    low_res_transform=low_res_transform\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=prefetch_factor,\n",
        "    persistent_workers=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
        "        self.act1 = nn.SiLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
        "        self.act2 = nn.SiLU()\n",
        "        self.residual_conv = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "        self.time_mlp = None\n",
        "        if time_emb_dim is not None:\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(time_emb_dim, out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, t_emb=None):\n",
        "        h = self.act1(self.norm1(self.conv1(x)))\n",
        "        if self.time_mlp is not None and t_emb is not None:\n",
        "            t_h = self.time_mlp(t_emb)\n",
        "            h = h + t_h[:, :, None, None]\n",
        "        h = self.act2(self.norm2(self.conv2(h)))\n",
        "        return h + self.residual_conv(x)\n",
        "\n",
        "class CrossResolutionAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.attn = BasicTransformerBlock(\n",
        "            dim=dim,\n",
        "            num_attention_heads=num_heads,\n",
        "            attention_head_dim=dim_head,\n",
        "            cross_attention_dim=dim,\n",
        "            only_cross_attention=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        context = rearrange(context, 'b c h w -> b (h w) c')\n",
        "        out = self.attn(hidden_states=x, encoder_hidden_states=context)\n",
        "        out = rearrange(out, 'b (h w) c -> b c h w', h=int(x.shape[1]**0.5))\n",
        "        return out\n",
        "\n",
        "class PixelShuffleUpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, scale_factor=2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels * (scale_factor ** 2), 3, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pixel_shuffle(self.conv(x))\n",
        "\n",
        "class ADISModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        base_dim=64,\n",
        "        time_emb_dim=256,\n",
        "        cra_blocks=3,\n",
        "        cra_dim=256,\n",
        "        cra_heads=8\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_emb = SinusoidalPosEmb(base_dim)\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(base_dim, time_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim)\n",
        "        )\n",
        "\n",
        "        self.noisy_encoder_in = ResidualBlock(in_channels, base_dim, time_emb_dim)\n",
        "        self.noisy_encoder_down1 = ResidualBlock(base_dim, base_dim * 2, time_emb_dim)\n",
        "\n",
        "        self.low_res_encoder_in = ResidualBlock(in_channels, base_dim)\n",
        "        self.low_res_encoder_down1 = ResidualBlock(base_dim, base_dim * 2)\n",
        "\n",
        "        self.low_res_feat_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        current_dim = base_dim * 2\n",
        "        self.noisy_encoder_down2 = ResidualBlock(current_dim, cra_dim, time_emb_dim)\n",
        "        self.low_res_encoder_down2 = ResidualBlock(current_dim, cra_dim)\n",
        "\n",
        "        self.low_res_feat_upsample_2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "        self.cra_blocks = nn.ModuleList([\n",
        "            CrossResolutionAttentionBlock(dim=cra_dim, num_heads=cra_heads)\n",
        "            for _ in range(cra_blocks)\n",
        "        ])\n",
        "\n",
        "        self.decoder_in = ResidualBlock(cra_dim, current_dim, time_emb_dim)\n",
        "\n",
        "        self.decoder_up1 = PixelShuffleUpBlock(current_dim, current_dim)\n",
        "        self.decoder_res1 = ResidualBlock(current_dim, base_dim, time_emb_dim)\n",
        "\n",
        "        self.decoder_up2 = PixelShuffleUpBlock(base_dim, base_dim)\n",
        "        self.decoder_res2 = ResidualBlock(base_dim, base_dim, time_emb_dim)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(base_dim, out_channels, 1)\n",
        "\n",
        "    def forward(self, noisy_image_u_t, timestep_t, low_res_image_u_low):\n",
        "        t_emb = self.time_emb(timestep_t)\n",
        "        t_emb = self.time_mlp(t_emb)\n",
        "\n",
        "        phi_d = self.low_res_encoder_in(low_res_image_u_low)\n",
        "        phi_d = self.low_res_encoder_down1(phi_d)\n",
        "\n",
        "        phi_u = self.noisy_encoder_in(noisy_image_u_t, t_emb)\n",
        "        phi_u = self.noisy_encoder_down1(phi_u, t_emb)\n",
        "\n",
        "        phi_d_upsampled = self.low_res_feat_upsample(phi_d)\n",
        "        phi_u = phi_u + phi_d_upsampled\n",
        "\n",
        "        phi_u_cra = self.noisy_encoder_down2(phi_u, t_emb)\n",
        "        phi_d_cra = self.low_res_encoder_down2(phi_d)\n",
        "\n",
        "        phi_d_cra_context = self.low_res_feat_upsample_2(phi_d_cra)\n",
        "\n",
        "        cra_out = phi_u_cra\n",
        "        for cra_block in self.cra_blocks:\n",
        "            cra_out = cra_block(cra_out, context=phi_d_cra_context)\n",
        "\n",
        "        dec = self.decoder_in(cra_out, t_emb)\n",
        "        dec = self.decoder_up1(dec)\n",
        "        dec = self.decoder_res1(dec, t_emb)\n",
        "        dec = self.decoder_up2(dec)\n",
        "        dec = self.decoder_res2(dec, t_emb)\n",
        "\n",
        "        predicted_noise = self.final_conv(dec)\n",
        "        return predicted_noise\n",
        "\n",
        "\n",
        "model = ADISModel(\n",
        "    in_channels=channels,\n",
        "    out_channels=channels,\n",
        "    base_dim=128,\n",
        "    time_emb_dim=512,\n",
        "    cra_blocks=num_cra_blocks,\n",
        "    cra_dim=512,\n",
        "    cra_heads=8\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "model_size = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {model_size:,}\")\n",
        "\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=num_train_timesteps,\n",
        "    beta_schedule=\"linear\",\n",
        "    prediction_type=\"epsilon\",\n",
        "    clip_sample=False\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=0.01,\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=(len(dataloader) * epochs) // gradient_accumulation_steps,\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=mixed_precision,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    log_with=\"tensorboard\",\n",
        "    project_dir=tensorboard_dir,\n",
        ")\n",
        "model, optimizer, dataloader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_and_save_images(epoch, model, scheduler, low_res_batch):\n",
        "    num_images = low_res_batch.shape[0]\n",
        "\n",
        "    sample = torch.randn(\n",
        "        num_images,\n",
        "        channels,\n",
        "        high_res_image_size,\n",
        "        high_res_image_size,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "    for t in tqdm(scheduler.timesteps, desc=\"Sampling\"):\n",
        "        noise_pred = model(\n",
        "            noisy_image_u_t=sample,\n",
        "            timestep_t=t.unsqueeze(0).repeat(num_images).to(device),\n",
        "            low_res_image_u_low=low_res_batch\n",
        "        )\n",
        "        sample = scheduler.step(noise_pred, t, sample).prev_sample\n",
        "\n",
        "    images = (sample / 2 + 0.5).clamp(0, 1)\n",
        "    image_grid = make_grid(images, nrow=int(math.sqrt(num_images)))\n",
        "\n",
        "    grid_image_path = f\"{output_dir}/samples/epoch_{epoch}.png\"\n",
        "    save_image(image_grid, grid_image_path)\n",
        "\n",
        "    writer.add_image(\"generated_images\", image_grid, epoch)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(transforms.ToPILImage()(image_grid))\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Epoch {epoch}\")\n",
        "    plt.show()\n",
        "\n",
        "def save_real_examples(dataloader, num_examples=9):\n",
        "    batch, _ = next(iter(dataloader))\n",
        "    high_res_images = batch[0][:num_examples]\n",
        "    low_res_images = batch[1][:num_examples]\n",
        "\n",
        "    high_res_images = (high_res_images / 2 + 0.5).clamp(0, 1)\n",
        "    low_res_images = (low_res_images / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "    image_grid_high = make_grid(high_res_images, nrow=3)\n",
        "    image_grid_low = make_grid(low_res_images, nrow=3)\n",
        "\n",
        "    save_image(image_grid_high, f\"{output_dir}/real_examples_high_res.png\")\n",
        "    save_image(image_grid_low, f\"{output_dir}/real_examples_low_res.png\")\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(transforms.ToPILImage()(image_grid_high))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Real Dataset Examples (High-Res 64x64)\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(transforms.ToPILImage()(image_grid_low))\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Real Dataset Examples (Low-Res 32x32)\")\n",
        "    plt.show()\n",
        "\n",
        "try:\n",
        "    save_real_examples(dataloader)\n",
        "except Exception as e:\n",
        "    print(f\"Error saving real examples: {e}\")\n",
        "\n",
        "dataloader_iter = iter(dataloader)\n",
        "\n",
        "global_step = 0\n",
        "final_model_state = None\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    start_time = torch.cuda.Event(enable_timing=True)\n",
        "    end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start_time.record()\n",
        "    progress_bar = tqdm(total=len(dataloader), desc=f\"Epoch {epoch}\")\n",
        "\n",
        "    for step, (batch_data, _) in enumerate(dataloader):\n",
        "        high_res_images, low_res_images = batch_data\n",
        "\n",
        "        if high_res_images.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        with accelerator.accumulate(model):\n",
        "            noise = torch.randn_like(high_res_images)\n",
        "            timesteps = torch.randint(\n",
        "                0, noise_scheduler.config.num_train_timesteps,\n",
        "                (high_res_images.shape[0],),\n",
        "                device=high_res_images.device\n",
        "            ).long()\n",
        "\n",
        "            noisy_images = noise_scheduler.add_noise(high_res_images, noise, timesteps)\n",
        "\n",
        "            noise_pred = model(\n",
        "                noisy_image_u_t=noisy_images,\n",
        "                timestep_t=timesteps,\n",
        "                low_res_image_u_low=low_res_images\n",
        "            )\n",
        "\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.detach().item()\n",
        "        progress_bar.update(1)\n",
        "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "        progress_bar.set_postfix(**logs)\n",
        "        global_step += 1\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            accelerator.log(logs, step=global_step)\n",
        "\n",
        "    end_time.record()\n",
        "    torch.cuda.synchronize()\n",
        "    epoch_time = start_time.elapsed_time(end_time) / 1000\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    writer.add_scalar(\"train/loss\", avg_loss, epoch)\n",
        "    writer.add_scalar(\"train/epoch_time\", epoch_time, epoch)\n",
        "\n",
        "    progress_bar.close()\n",
        "    print(f\"Epoch {epoch} completed in {epoch_time:.2f}s, Avg loss: {avg_loss:.4f}\")\n",
        "\n",
        "    if (epoch + 1) % save_image_epochs == 0 or epoch == epochs - 1:\n",
        "        unwrapped_model = accelerator.unwrap_model(model)\n",
        "        unwrapped_model.eval()\n",
        "\n",
        "        try:\n",
        "            eval_batch, _ = next(dataloader_iter)\n",
        "        except StopIteration:\n",
        "            dataloader_iter = iter(dataloader)\n",
        "            eval_batch, _ = next(dataloader_iter)\n",
        "\n",
        "        eval_low_res = eval_batch[1][:eval_batch_size].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sample_and_save_images(epoch + 1, unwrapped_model, noise_scheduler, eval_low_res)\n",
        "\n",
        "        if epoch == epochs - 1:\n",
        "            final_model_state = unwrapped_model.state_dict()\n",
        "\n",
        "        model.train()\n",
        "\n",
        "writer.close()\n",
        "print(\"Training completed!\")\n",
        "\n",
        "\n",
        "if final_model_state:\n",
        "    final_model = ADISModel(\n",
        "        in_channels=channels,\n",
        "        out_channels=channels,\n",
        "        base_dim=128,\n",
        "        time_emb_dim=512,\n",
        "        cra_blocks=num_cra_blocks,\n",
        "        cra_dim=512,\n",
        "        cra_heads=8\n",
        "    )\n",
        "    final_model.load_state_dict(final_model_state)\n",
        "    final_model.to(device)\n",
        "    final_model.eval()\n",
        "\n",
        "    final_scheduler = noise_scheduler\n",
        "\n",
        "    num_samples = 16\n",
        "\n",
        "    try:\n",
        "        final_eval_batch, _ = next(dataloader_iter)\n",
        "    except StopIteration:\n",
        "        dataloader_iter = iter(dataloader)\n",
        "        final_eval_batch, _ = next(dataloader_iter)\n",
        "\n",
        "    final_low_res_batch = final_eval_batch[1][:num_samples].to(device)\n",
        "\n",
        "    sample = torch.randn(\n",
        "        num_samples,\n",
        "        channels,\n",
        "        high_res_image_size,\n",
        "        high_res_image_size,\n",
        "        device=device\n",
        "    )\n",
        "    final_scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "    for t in tqdm(final_scheduler.timesteps, desc=\"Final Generation\"):\n",
        "        with torch.no_grad():\n",
        "            noise_pred = final_model(\n",
        "                noisy_image_u_t=sample,\n",
        "                timestep_t=t.unsqueeze(0).repeat(num_samples).to(device),\n",
        "                low_res_image_u_low=final_low_res_batch\n",
        "            )\n",
        "        sample = final_scheduler.step(noise_pred, t, sample).prev_sample\n",
        "\n",
        "    sample_images_tensors = (sample / 2 + 0.5).clamp(0, 1)\n",
        "    sample_images = [transforms.ToPILImage()(img) for img in sample_images_tensors]\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "    for i, image in enumerate(sample_images):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(\"Final Generated Samples (Conditioned on Low-Res)\", y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def visualize_denoising(model, scheduler, low_res_image, num_steps=10):\n",
        "        sample = torch.randn(1, channels, high_res_image_size, high_res_image_size).to(device)\n",
        "        low_res_image = low_res_image.unsqueeze(0).to(device)\n",
        "\n",
        "        scheduler.set_timesteps(num_steps)\n",
        "\n",
        "        images = []\n",
        "        for t in tqdm(scheduler.timesteps, desc=\"Visualizing Denoising\"):\n",
        "            noise_pred = model(\n",
        "                noisy_image_u_t=sample,\n",
        "                timestep_t=t.unsqueeze(0).to(device),\n",
        "                low_res_image_u_low=low_res_image\n",
        "            )\n",
        "            sample = scheduler.step(noise_pred, t, sample).prev_sample\n",
        "\n",
        "            image = (sample / 2 + 0.5).clamp(0, 1)\n",
        "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
        "            images.append(image)\n",
        "\n",
        "        plt.figure(figsize=(15, 4))\n",
        "        for i, img in enumerate(images):\n",
        "            plt.subplot(1, num_steps, i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Step {i+1}\")\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.suptitle(\"Denoising Process\", y=1.05)\n",
        "        plt.show()\n",
        "\n",
        "    visualize_denoising(final_model, final_scheduler, final_low_res_batch[0], num_steps=10)\n",
        "\n",
        "else:\n",
        "    print(\"Final model state was not captured. Skipping final generation/visualization.\")"
      ]
    }
  ]
}