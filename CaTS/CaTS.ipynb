{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBaf69v4FCG_"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"CaTS_Framework_Pytorch.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1... (Replace with actual Colab link if desired)\n",
        "\n",
        "# Causality-Driven Teacher-Student (CaTS) Framework Implementation\n",
        "\n",
        "This notebook implements the CaTS framework based on the paper \"Consistency-Regularized Causal Teacher-Student Learning with Diffusion Synthesis for PCOS Diagnosis\". It integrates causal feature disentanglement within a semi-supervised Mean Teacher architecture.\n",
        "\"\"\"\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Setup and Imports\n",
        "\n",
        "# %%\n",
        "!pip install torch torchvision torchaudio tqdm pandas scikit-learn tensorboard Pillow -q\n",
        "\n",
        "# %%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, Sampler\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import itertools\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Seed everything for reproducibility\n",
        "def seed_everything(seed=1337):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
        "    # Set PyTorch deterministic operations for cudnn backend\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Configuration\n",
        "\n",
        "# %%\n",
        "# --- Hyperparameters ---\n",
        "config = {\n",
        "    \"image_size\": 256,\n",
        "    \"batch_size\": 16, # Total batch size\n",
        "    \"labeled_bs_ratio\": 0.5, # Ratio of labeled examples per batch (paper uses 20% labeled data overall, this controls batch composition)\n",
        "    \"num_classes\": 2, # PCOS Positive/Negative\n",
        "    \"base_lr\": 1e-3, # Paper: max 1e-3\n",
        "    \"weight_decay\": 1e-2, # Paper: 1e-2\n",
        "    \"epochs\": 100, # Number of training epochs\n",
        "    \"ema_decay\": 0.99, # For Teacher model update\n",
        "    \"consistency_lambda\": 1.0, # Weight for consistency loss (L_CR)\n",
        "    \"confound_lambda\": 1.0, # Weight for confounding suppression loss (L_con)\n",
        "    \"backdoor_lambda\": 1.0, # Weight for backdoor adjustment loss (L_b)\n",
        "    \"n_transformer_layers\": 6, # Paper: nL=6\n",
        "    \"n_causal_queries\": 8, # Paper: nQ=8\n",
        "    \"transformer_embed_dim\": 2048, # Match ResNet output channels\n",
        "    \"transformer_nhead\": 8, # Standard choice\n",
        "    \"transformer_ff_dim\": 2048, # Feed-forward dim\n",
        "    \"memory_bank_size\": 1024, # Size of the memory bank for L_b\n",
        "    \"train_test_split_ratio\": 0.8,\n",
        "    \"labeled_unlabeled_split_ratio\": 0.2, # Paper: 20% labeled in training set\n",
        "    \"seed\": 1337,\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"data_root\": \"/content/PCOSGen-train/images\", # Adjust if necessary\n",
        "    \"output_dir\": \"./cats_output\"\n",
        "}\n",
        "\n",
        "config[\"labeled_bs\"] = int(config[\"batch_size\"] * config[\"labeled_bs_ratio\"]) # Actual labeled samples per batch\n",
        "\n",
        "print(f\"Using device: {config['device']}\")\n",
        "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
        "os.makedirs(os.path.join(config[\"output_dir\"], \"checkpoints\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(config[\"output_dir\"], \"logs\"), exist_ok=True)\n",
        "\n",
        "# Tensorboard Writer\n",
        "writer = SummaryWriter(log_dir=os.path.join(config[\"output_dir\"], \"logs\"))\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Data Loading and Preparation\n",
        "\n",
        "# %% [markdown]\n",
        "# #### Unzip Data (Run Once)\n",
        "\n",
        "# %%\n",
        "# Mount Google Drive if data is there\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !unzip /content/drive/MyDrive/PCOSGen-train.zip -d /content/\n",
        "\n",
        "# If data is uploaded directly or already present, adjust path in config[\"data_root\"]\n",
        "\n",
        "# Create CSV file from folder structure (Run Once)\n",
        "if not os.path.exists('/content/pcos_dataset.csv'):\n",
        "    print(\"Creating dataset CSV...\")\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    class_map = {'PCOS_positive': 1, 'PCOS_negative': 0} # Example mapping\n",
        "    # Adjust class folder names based on your actual unzipped structure\n",
        "    positive_folder = os.path.join(config[\"data_root\"], '../PCOSGen-train/images') # Adjust if structure is different\n",
        "    negative_folder = os.path.join(config[\"data_root\"], '../PCOSGen-train/images') # Adjust if structure is different\n",
        "\n",
        "    # Find relevant image files based on naming convention or structure\n",
        "    all_files = [f for f in os.listdir(positive_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # Heuristic based on common naming patterns (adjust if needed)\n",
        "    for filename in all_files:\n",
        "        full_path = os.path.join(positive_folder, filename)\n",
        "        if 'pco' in filename.lower() or 'polycystic' in filename.lower() or 'infected' in filename.lower():\n",
        "             # Check if the file exists before adding\n",
        "             if os.path.exists(full_path):\n",
        "                 image_paths.append(full_path)\n",
        "                 labels.append(1) # PCOS Positive/Infected\n",
        "        elif 'normal' in filename.lower() or 'notinfected' in filename.lower():\n",
        "             # Check if the file exists before adding\n",
        "            if os.path.exists(full_path):\n",
        "                image_paths.append(full_path)\n",
        "                labels.append(0) # PCOS Negative/Not Infected\n",
        "\n",
        "    if not image_paths:\n",
        "         raise FileNotFoundError(\"No images found. Check data_root and folder structure.\")\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "    df.to_csv('/content/pcos_dataset.csv', index=False)\n",
        "    print(\"Dataset CSV created.\")\n",
        "else:\n",
        "    print(\"Dataset CSV already exists.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# #### Dataset Class and Augmentations\n",
        "\n",
        "# %%\n",
        "# Define augmentations\n",
        "# Weak Augmentation (for Teacher)\n",
        "weak_transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(config[\"image_size\"], padding=int(config[\"image_size\"]*0.125), padding_mode='reflect'),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Strong Augmentation (for Student on unlabeled data) - using RandAugment\n",
        "strong_transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.RandAugment(num_ops=2, magnitude=10), # Apply RandAugment\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Standard transform for validation/test\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "class PCOSImageDataset(Dataset):\n",
        "    def __init__(self, csv_file, weak_transform=None, strong_transform=None, val_transform=None, mode='train'):\n",
        "        self.dataframe = pd.read_csv(csv_file)\n",
        "        self.weak_transform = weak_transform\n",
        "        self.strong_transform = strong_transform\n",
        "        self.val_transform = val_transform\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx]['image_path']\n",
        "        label = torch.tensor(self.dataframe.iloc[idx]['label'], dtype=torch.long)\n",
        "\n",
        "        try:\n",
        "             # Check if the image path exists\n",
        "            if not os.path.exists(img_path):\n",
        "                 print(f\"Warning: Image path not found {img_path}. Skipping.\")\n",
        "                 # Return placeholders or handle appropriately\n",
        "                 placeholder_img = torch.zeros((3, config[\"image_size\"], config[\"image_size\"]))\n",
        "                 # Depending on mode, return expected tuple structure\n",
        "                 if self.mode == 'train':\n",
        "                    return placeholder_img, placeholder_img, torch.tensor(-1, dtype=torch.long) # Invalid label\n",
        "                 else:\n",
        "                    return placeholder_img, torch.tensor(-1, dtype=torch.long)\n",
        "\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                img_weak = self.weak_transform(image)\n",
        "                img_strong = self.strong_transform(image)\n",
        "                return img_weak, img_strong, label\n",
        "            else: # val or test\n",
        "                img_val = self.val_transform(image)\n",
        "                return img_val, label\n",
        "        except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "                placeholder_img = torch.zeros((3, config[\"image_size\"], config[\"image_size\"]))\n",
        "                if self.mode == 'train':\n",
        "                    return placeholder_img, placeholder_img, torch.tensor(-1, dtype=torch.long) # Invalid label\n",
        "                else:\n",
        "                    return placeholder_img, torch.tensor(-1, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# #### Data Splitting and Sampler\n",
        "\n",
        "# %%\n",
        "# Load full dataset info\n",
        "full_df = pd.read_csv('/content/pcos_dataset.csv')\n",
        "\n",
        "# Split into Train (80%) and Test (20%)\n",
        "train_val_indices, test_indices = train_test_split(\n",
        "    range(len(full_df)),\n",
        "    test_size=1.0 - config[\"train_test_split_ratio\"],\n",
        "    stratify=full_df['label'],\n",
        "    random_state=config[\"seed\"]\n",
        ")\n",
        "\n",
        "# Split Train into actual Train and Validation\n",
        "train_indices, val_indices = train_test_split(\n",
        "    train_val_indices,\n",
        "    test_size=0.2, # 20% of the 80% train_val set for validation\n",
        "    stratify=full_df.iloc[train_val_indices]['label'],\n",
        "    random_state=config[\"seed\"]\n",
        ")\n",
        "\n",
        "# Further split Train indices into Labeled and Unlabeled\n",
        "labeled_indices, unlabeled_indices = train_test_split(\n",
        "    train_indices,\n",
        "    test_size=1.0 - config[\"labeled_unlabeled_split_ratio\"],\n",
        "    stratify=full_df.iloc[train_indices]['label'],\n",
        "    random_state=config[\"seed\"]\n",
        ")\n",
        "\n",
        "print(f\"Total samples: {len(full_df)}\")\n",
        "print(f\"Train samples: {len(train_indices)} (Labeled: {len(labeled_indices)}, Unlabeled: {len(unlabeled_indices)})\")\n",
        "print(f\"Validation samples: {len(val_indices)}\")\n",
        "print(f\"Test samples: {len(test_indices)}\")\n",
        "\n",
        "# Create dataset instances\n",
        "train_dataset = PCOSImageDataset(csv_file='/content/pcos_dataset.csv', weak_transform=weak_transform, strong_transform=strong_transform, mode='train')\n",
        "val_dataset = PCOSImageDataset(csv_file='/content/pcos_dataset.csv', val_transform=val_transform, mode='val')\n",
        "test_dataset = PCOSImageDataset(csv_file='/content/pcos_dataset.csv', val_transform=val_transform, mode='test')\n",
        "\n",
        "# Create subset datasets\n",
        "val_subset = Subset(val_dataset, val_indices)\n",
        "test_subset = Subset(test_dataset, test_indices)\n",
        "\n",
        "# --- Sampler for Training ---\n",
        "class TwoStreamBatchSampler(Sampler):\n",
        "    \"\"\"Iterate two sets of indices\n",
        "\n",
        "    An 'epoch' is one iteration through the primary indices.\n",
        "    During the epoch, the secondary indices are iterated through\n",
        "    as many times as needed.\n",
        "    \"\"\"\n",
        "    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n",
        "        self.primary_indices = primary_indices # Labeled indices\n",
        "        self.secondary_indices = secondary_indices # Unlabeled indices\n",
        "        self.secondary_batch_size = secondary_batch_size # Unlabeled batch size\n",
        "        self.primary_batch_size = batch_size - secondary_batch_size # Labeled batch size\n",
        "\n",
        "        assert len(self.primary_indices) >= self.primary_batch_size > 0\n",
        "        assert len(self.secondary_indices) >= self.secondary_batch_size >= 0 # Allow 0 unlabeled\n",
        "\n",
        "        print(f\"Sampler: Total Batch={batch_size}, Labeled={self.primary_batch_size}, Unlabeled={self.secondary_batch_size}\")\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        primary_iter = iterate_once(self.primary_indices)\n",
        "        secondary_iter = iterate_eternally(self.secondary_indices)\n",
        "        len_primary_batches = len(self.primary_indices) // self.primary_batch_size\n",
        "        len_secondary_batches = len(self.secondary_indices) // self.secondary_batch_size if self.secondary_batch_size > 0 else float('inf')\n",
        "\n",
        "        num_batches = min(len_primary_batches, len_secondary_batches)\n",
        "        if self.secondary_batch_size == 0 : # Handle fully supervised case\n",
        "            num_batches = len_primary_batches\n",
        "\n",
        "        # Combine batches ensuring the total length matches the minimum iterations possible\n",
        "        combined_iter = (\n",
        "            primary_batch + secondary_batch\n",
        "            for (primary_batch, secondary_batch)\n",
        "            in zip(grouper(primary_iter, self.primary_batch_size),\n",
        "                    grouper(secondary_iter, self.secondary_batch_size))\n",
        "        )\n",
        "\n",
        "        # Truncate to the number of batches determined by the primary indices (labeled data dictates epoch size)\n",
        "        # Or handle fully supervised case where secondary_batch_size is 0\n",
        "        if self.secondary_batch_size > 0:\n",
        "            return itertools.islice(combined_iter, num_batches)\n",
        "        else:\n",
        "            # Fully supervised: only primary batches\n",
        "            primary_only_iter = (primary_batch for primary_batch in grouper(primary_iter, self.primary_batch_size))\n",
        "            return itertools.islice(primary_only_iter, num_batches)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        len_primary_batches = len(self.primary_indices) // self.primary_batch_size\n",
        "        if self.secondary_batch_size == 0:\n",
        "            return len_primary_batches\n",
        "        len_secondary_batches = len(self.secondary_indices) // self.secondary_batch_size if self.secondary_batch_size > 0 else float('inf')\n",
        "        return min(len_primary_batches, len_secondary_batches)\n",
        "\n",
        "\n",
        "def iterate_once(iterable):\n",
        "    return np.random.permutation(iterable)\n",
        "\n",
        "\n",
        "def iterate_eternally(indices):\n",
        "    def infinite_shuffles():\n",
        "        while True:\n",
        "            yield np.random.permutation(indices)\n",
        "    return itertools.chain.from_iterable(infinite_shuffles())\n",
        "\n",
        "\n",
        "def grouper(iterable, n):\n",
        "    \"Collect data into fixed-length chunks or blocks\"\n",
        "    # grouper('ABCDEFG', 3) --> ABC DEF\"\n",
        "    args = [iter(iterable)] * n\n",
        "    # Use zip_longest to handle the last batch if iterable length is not divisible by n\n",
        "    # Fill value is not strictly needed here as we handle the last batch explicitly if necessary\n",
        "    # Or, the sampler length logic handles truncation. Sticking to zip for simplicity based on original.\n",
        "    return zip(*args)\n",
        "\n",
        "# Create the sampler and dataloaders\n",
        "train_batch_sampler = TwoStreamBatchSampler(\n",
        "    labeled_indices, unlabeled_indices, config[\"batch_size\"], config[\"batch_size\"] - config[\"labeled_bs\"]\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_sampler=train_batch_sampler, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "print(f\"Train loader batches: {len(train_loader)}\")\n",
        "print(f\"Val loader batches: {len(val_loader)}\")\n",
        "print(f\"Test loader batches: {len(test_loader)}\")\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Model Architecture (CaTS)\n",
        "\n",
        "# %%\n",
        "# --- Attention Modules (from Causal.ipynb) ---\n",
        "class PositionAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(PositionAttention, self).__init__()\n",
        "        # Reduce channels for query, key, value to avoid excessive computation\n",
        "        reduced_channels = max(in_channels // 8, 64) # Ensure at least 64 channels\n",
        "        self.query_conv = nn.Conv2d(in_channels, reduced_channels, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, reduced_channels, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, height, width = x.size()\n",
        "        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n",
        "\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, height, width)\n",
        "\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax  = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        batch_size, C, height, width = x.size()\n",
        "        proj_query = x.view(batch_size, C, -1)\n",
        "        proj_key = x.view(batch_size, C, -1).permute(0, 2, 1)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        # Optional: Add scaling based on channel dimension if needed\n",
        "        # energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = x.view(batch_size, C, -1)\n",
        "\n",
        "        out = torch.bmm(attention, proj_value)\n",
        "        out = out.view(batch_size, C, height, width)\n",
        "\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "\n",
        "# --- Feature Extractor (Representation Learning Block) ---\n",
        "# Adapted from Causal.ipynb and cats.jpg (b) diagram\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, in_channels=2048, out_channels=1024):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.conv_ch = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.ch_attn = ChannelAttention(out_channels) # Applied on reduced channels\n",
        "\n",
        "        self.conv_pos = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.pos_attn = PositionAttention(out_channels) # Applied on reduced channels\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.conv_gap = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.conv_skip = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "        # Batch Norm and ReLU can be added for stability\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Branch 1 (Attention)\n",
        "        x_ch = self.conv_ch(x)\n",
        "        x_ch_attn = self.ch_attn(x_ch)\n",
        "\n",
        "        x_pos = self.conv_pos(x)\n",
        "        x_pos_attn = self.pos_attn(x_pos)\n",
        "\n",
        "        branch1_out = self.relu(self.bn1(x_ch_attn + x_pos_attn))\n",
        "\n",
        "        # Branch 2 (GAP + Skip)\n",
        "        x_gap = self.gap(x)\n",
        "        x_gap = self.conv_gap(x_gap) # Shape: (B, out_channels, 1, 1)\n",
        "\n",
        "        x_skip = self.conv_skip(x) # Shape: (B, out_channels, H, W)\n",
        "\n",
        "        # Element-wise multiplication after broadcasting GAP features\n",
        "        branch2_out = self.relu(self.bn1(x_skip * x_gap.expand_as(x_skip) + x_skip)) # Modified based on common patterns\n",
        "\n",
        "        # Concatenate\n",
        "        out = torch.cat((branch1_out, branch2_out), dim=1) # Output channels = 2 * out_channels\n",
        "        return out\n",
        "\n",
        "# --- Positional Encoding for Transformer ---\n",
        "class FixedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=64): # max_len = H*W of feature map\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0) # Shape: (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, SeqLen, Dim) where SeqLen = H*W\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "\n",
        "# --- Causal Disentanglement with Transformer Decoder ---\n",
        "class CausalDisentanglement(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_decoder_layers, dim_feedforward, n_queries):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_queries = n_queries\n",
        "\n",
        "        self.pos_encoder = FixedPositionalEncoding(d_model)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # Learnable causal queries\n",
        "        self.causal_queries = nn.Parameter(torch.zeros(1, n_queries, d_model))\n",
        "\n",
        "        # Layer for extracting attention weights (might need custom implementation or hooks)\n",
        "        # For simplicity, we'll try to get weights if using nn.MultiheadAttention directly,\n",
        "        # otherwise, this part might require modification. Let's start without explicit weight extraction.\n",
        "\n",
        "    def forward(self, features):\n",
        "        # features shape: (B, C, H, W)\n",
        "        B, C, H, W = features.shape\n",
        "        features_flat = features.flatten(2).permute(0, 2, 1) # Shape: (B, H*W, C)\n",
        "\n",
        "        # Add positional encoding\n",
        "        features_pos = self.pos_encoder(features_flat)\n",
        "\n",
        "        # Prepare queries\n",
        "        queries = self.causal_queries.repeat(B, 1, 1) # Shape: (B, n_queries, d_model)\n",
        "\n",
        "        # Pass through transformer decoder\n",
        "        # `memory` is the encoder output (our features_pos)\n",
        "        # `tgt` is the input to the decoder (our queries)\n",
        "        causal_output = self.transformer_decoder(tgt=queries, memory=features_pos)\n",
        "        # causal_output shape: (B, n_queries, d_model)\n",
        "\n",
        "        # --- Calculate Confounding Features ---\n",
        "        # This part is tricky without direct access to attention weights per layer.\n",
        "        # Approximation: Use the final causal output to mask the input features.\n",
        "        # Project causal output back to spatial dimension attention map.\n",
        "        # This is a simplification/alternative to Eq 13.\n",
        "        # Another approach: Average causal_output across queries and subtract from input features.\n",
        "\n",
        "        # Simplified approach: Average causal features, assume confound is the residual\n",
        "        F_cau_avg = causal_output.mean(dim=1) # Shape: (B, d_model)\n",
        "\n",
        "        # Average spatial features as a baseline representation\n",
        "        S_avg = features_pos.mean(dim=1) # Shape: (B, d_model)\n",
        "\n",
        "        # Confounding features (conceptual approximation)\n",
        "        F_con = S_avg - F_cau_avg # Residual features\n",
        "\n",
        "        # Return averaged features for MLPs\n",
        "        return F_cau_avg, F_con\n",
        "\n",
        "\n",
        "# --- MLP Heads ---\n",
        "class MLPHead(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=512, out_dim=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x_intermediate = self.relu(x) # Store intermediate features (eta)\n",
        "        x = self.dropout(x_intermediate)\n",
        "        x_logits = self.fc2(x) # Final logits (mu)\n",
        "        return x_intermediate, x_logits\n",
        "\n",
        "\n",
        "# --- Complete CaTS Model ---\n",
        "class CaTSModel(nn.Module):\n",
        "    def __init__(self, num_classes, n_transformer_layers, n_causal_queries,\n",
        "                 transformer_embed_dim, transformer_nhead, transformer_ff_dim, dropout=0.5):\n",
        "        super().__init__()\n",
        "        # Backbone (ResNet101) - freeze layers initially\n",
        "        resnet = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False # Freeze backbone\n",
        "        self.backbone_out_channels = 2048\n",
        "\n",
        "        # Representation Learning\n",
        "        self.feature_extractor = FeatureExtractor(in_channels=self.backbone_out_channels, out_channels=self.backbone_out_channels // 2)\n",
        "        self.feature_extractor_out_channels = self.backbone_out_channels # Because of concatenation\n",
        "\n",
        "        # Causal Disentanglement\n",
        "        self.causal_disentanglement = CausalDisentanglement(\n",
        "            d_model=self.feature_extractor_out_channels,\n",
        "            nhead=transformer_nhead,\n",
        "            num_decoder_layers=n_transformer_layers,\n",
        "            dim_feedforward=transformer_ff_dim,\n",
        "            n_queries=n_causal_queries\n",
        "        )\n",
        "        self.causal_out_dim = self.feature_extractor_out_channels\n",
        "\n",
        "        # MLP Heads\n",
        "        self.mlp_cau = MLPHead(in_dim=self.causal_out_dim, out_dim=num_classes, dropout=dropout)\n",
        "        self.mlp_con = MLPHead(in_dim=self.causal_out_dim, out_dim=num_classes, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        # print(\"Backbone out:\", x.shape) # Should be [B, 2048, 8, 8] for 256x256 input\n",
        "        x = self.feature_extractor(x)\n",
        "        # print(\"Feature Extractor out:\", x.shape) # Should be [B, 2048, 8, 8]\n",
        "        F_cau_vec, F_con_vec = self.causal_disentanglement(x)\n",
        "        # print(\"Causal/Confound Vecs:\", F_cau_vec.shape, F_con_vec.shape) # Should be [B, 2048]\n",
        "\n",
        "        eta_cau, mu_cau = self.mlp_cau(F_cau_vec) # Causal intermediate features and logits\n",
        "        eta_con, mu_con = self.mlp_con(F_con_vec) # Confound intermediate features and logits\n",
        "\n",
        "        return eta_cau, eta_con, mu_cau, mu_con\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Loss Functions\n",
        "\n",
        "# %%\n",
        "# --- Causal Loss (L_cau) ---\n",
        "causal_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Confounding Suppression Loss (L_con) ---\n",
        "kl_loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
        "uniform_dist = torch.full((config[\"batch_size\"], config[\"num_classes\"]), 1.0 / config[\"num_classes\"]).to(config[\"device\"])\n",
        "\n",
        "def confound_loss_fn(mu_con_stu):\n",
        "    # Ensure uniform_dist matches batch size\n",
        "    current_batch_size = mu_con_stu.shape[0]\n",
        "    if uniform_dist.shape[0] != current_batch_size:\n",
        "        u_dist = torch.full((current_batch_size, config[\"num_classes\"]), 1.0 / config[\"num_classes\"]).to(config[\"device\"])\n",
        "    else:\n",
        "        u_dist = uniform_dist[:current_batch_size]\n",
        "\n",
        "    log_softmax_mu_con = F.log_softmax(mu_con_stu, dim=1)\n",
        "    return kl_loss_fn(log_softmax_mu_con, u_dist)\n",
        "\n",
        "# --- Backdoor Adjustment Loss (L_b) ---\n",
        "backdoor_loss_fn = nn.CrossEntropyLoss()\n",
        "# Memory bank for confounding features (eta_con) from labeled data\n",
        "confound_memory_bank = []\n",
        "memory_bank_labels = []\n",
        "\n",
        "def update_memory_bank(eta_con_stu_labeled, labels_labeled):\n",
        "    global confound_memory_bank, memory_bank_labels\n",
        "    confound_memory_bank.append(eta_con_stu_labeled.detach().cpu())\n",
        "    memory_bank_labels.append(labels_labeled.detach().cpu())\n",
        "    # Keep bank size limited\n",
        "    if len(confound_memory_bank) > config[\"memory_bank_size\"] // config[\"labeled_bs\"]:\n",
        "         confound_memory_bank.pop(0)\n",
        "         memory_bank_labels.pop(0)\n",
        "\n",
        "\n",
        "def sample_from_memory_bank(current_labels):\n",
        "    if not confound_memory_bank:\n",
        "        return None # Return None if bank is empty\n",
        "\n",
        "    # Concatenate stored features and labels\n",
        "    all_eta_con = torch.cat(confound_memory_bank, dim=0)\n",
        "    all_labels = torch.cat(memory_bank_labels, dim=0)\n",
        "\n",
        "    sampled_eta_con = []\n",
        "    current_device = current_labels.device\n",
        "\n",
        "    # Stratified Random Sampling (SRS)\n",
        "    for i in range(len(current_labels)):\n",
        "        label = current_labels[i].item()\n",
        "        # Find indices in memory bank with the same label\n",
        "        eligible_indices = (all_labels == label).nonzero(as_tuple=True)[0]\n",
        "        if len(eligible_indices) > 0:\n",
        "            # Randomly sample one index\n",
        "            sampled_idx = random.choice(eligible_indices)\n",
        "            sampled_eta_con.append(all_eta_con[sampled_idx])\n",
        "        else:\n",
        "             # If no matching label found, sample randomly from the whole bank\n",
        "            if len(all_eta_con) > 0:\n",
        "                 sampled_idx = random.randrange(len(all_eta_con))\n",
        "                 sampled_eta_con.append(all_eta_con[sampled_idx])\n",
        "            else:\n",
        "                 # Should not happen if bank is not empty, but as a fallback:\n",
        "                 sampled_eta_con.append(torch.zeros_like(all_eta_con[0])) # Append zeros\n",
        "\n",
        "\n",
        "    if not sampled_eta_con:\n",
        "         return None\n",
        "\n",
        "    return torch.stack(sampled_eta_con).to(current_device)\n",
        "\n",
        "\n",
        "# --- Consistency Loss (L_CR) ---\n",
        "consistency_loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Training Setup\n",
        "\n",
        "# %%\n",
        "# --- Initialize Models ---\n",
        "student_model = CaTSModel(\n",
        "    num_classes=config[\"num_classes\"],\n",
        "    n_transformer_layers=config[\"n_transformer_layers\"],\n",
        "    n_causal_queries=config[\"n_causal_queries\"],\n",
        "    transformer_embed_dim=config[\"transformer_embed_dim\"],\n",
        "    transformer_nhead=config[\"transformer_nhead\"],\n",
        "    transformer_ff_dim=config[\"transformer_ff_dim\"],\n",
        "    dropout=0.5\n",
        ").to(config[\"device\"])\n",
        "\n",
        "teacher_model = CaTSModel(\n",
        "    num_classes=config[\"num_classes\"],\n",
        "    n_transformer_layers=config[\"n_transformer_layers\"],\n",
        "    n_causal_queries=config[\"n_causal_queries\"],\n",
        "    transformer_embed_dim=config[\"transformer_embed_dim\"],\n",
        "    transformer_nhead=config[\"transformer_nhead\"],\n",
        "    transformer_ff_dim=config[\"transformer_ff_dim\"],\n",
        "    dropout=0.5\n",
        ").to(config[\"device\"])\n",
        "\n",
        "# Initialize teacher with student weights and detach parameters\n",
        "teacher_model.load_state_dict(student_model.state_dict())\n",
        "for param in teacher_model.parameters():\n",
        "    param.detach_()\n",
        "\n",
        "\n",
        "# --- Optimizer ---\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=config[\"base_lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "# --- EMA Update Function ---\n",
        "def update_ema_variables(model, ema_model, alpha, global_step):\n",
        "    # alpha is the EMA decay parameter (e.g., 0.99)\n",
        "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
        "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
        "        ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
        "\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Training Loop\n",
        "\n",
        "# %%\n",
        "print(\"Starting Training...\")\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "    student_model.train()\n",
        "    teacher_model.eval() # Teacher is always in eval mode\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_loss_cau = 0.0\n",
        "    epoch_loss_con = 0.0\n",
        "    epoch_loss_bd = 0.0\n",
        "    epoch_loss_cr = 0.0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\", leave=False)\n",
        "\n",
        "    for batch_idx, (img_weak, img_strong, labels) in enumerate(progress_bar):\n",
        "        img_weak = img_weak.to(config[\"device\"])\n",
        "        img_strong = img_strong.to(config[\"device\"])\n",
        "        labels = labels.to(config[\"device\"])\n",
        "\n",
        "        # Split batch into labeled and unlabeled\n",
        "        # Note: The sampler ensures the first `labeled_bs` are labeled indices\n",
        "        img_weak_lab, img_strong_lab = img_weak[:config[\"labeled_bs\"]], img_strong[:config[\"labeled_bs\"]]\n",
        "        img_weak_unlab, img_strong_unlab = img_weak[config[\"labeled_bs\"]:], img_strong[config[\"labeled_bs\"]:]\n",
        "        labels_lab = labels[:config[\"labeled_bs\"]]\n",
        "\n",
        "        # ---------------------------------\n",
        "        # Supervised Loss Calculation (L_sup) on labeled data\n",
        "        # ---------------------------------\n",
        "        loss_sup = torch.tensor(0.0).to(config[\"device\"])\n",
        "        loss_cau = torch.tensor(0.0).to(config[\"device\"])\n",
        "        loss_con = torch.tensor(0.0).to(config[\"device\"])\n",
        "        loss_bd = torch.tensor(0.0).to(config[\"device\"])\n",
        "\n",
        "        if config[\"labeled_bs\"] > 0:\n",
        "            # Use weak augmentation for supervised training part\n",
        "            eta_cau_stu_lab, eta_con_stu_lab, mu_cau_stu_lab, mu_con_stu_lab = student_model(img_weak_lab)\n",
        "\n",
        "            # 1. Causal Loss (L_cau)\n",
        "            loss_cau = causal_loss_fn(mu_cau_stu_lab, labels_lab)\n",
        "\n",
        "            # 2. Confounding Suppression Loss (L_con)\n",
        "            loss_con = confound_loss_fn(mu_con_stu_lab)\n",
        "\n",
        "            # Update memory bank for L_b\n",
        "            update_memory_bank(eta_con_stu_lab, labels_lab)\n",
        "\n",
        "            # 3. Backdoor Adjustment Loss (L_b)\n",
        "            sampled_eta_con = sample_from_memory_bank(labels_lab)\n",
        "            if sampled_eta_con is not None and sampled_eta_con.shape[0] == eta_cau_stu_lab.shape[0]:\n",
        "                # Perturb causal features\n",
        "                eta_cau_perturbed = eta_cau_stu_lab + sampled_eta_con\n",
        "                # Pass *perturbed intermediate features* through the rest of the causal MLP\n",
        "                # Re-apply layers after the intermediate feature extraction in MLP_cau\n",
        "                x_perturbed = student_model.mlp_cau.dropout(eta_cau_perturbed) # Assuming eta is after ReLU\n",
        "                mu_cau_perturbed = student_model.mlp_cau.fc2(x_perturbed)\n",
        "\n",
        "                loss_bd = backdoor_loss_fn(mu_cau_perturbed, labels_lab)\n",
        "            else:\n",
        "                 loss_bd = torch.tensor(0.0).to(config[\"device\"]) # Skip if bank is empty or size mismatch\n",
        "\n",
        "\n",
        "            # Total Supervised Loss\n",
        "            loss_sup = loss_cau + config[\"confound_lambda\"] * loss_con + config[\"backdoor_lambda\"] * loss_bd\n",
        "\n",
        "        # ---------------------------------\n",
        "        # Consistency Loss Calculation (L_CR) on unlabeled data\n",
        "        # ---------------------------------\n",
        "        loss_cr = torch.tensor(0.0).to(config[\"device\"])\n",
        "        num_unlabeled = img_strong_unlab.shape[0]\n",
        "\n",
        "        if num_unlabeled > 0:\n",
        "            # Student forward pass on strong augmentation\n",
        "            _, _, mu_cau_stu_unlab, _ = student_model(img_strong_unlab)\n",
        "\n",
        "            # Teacher forward pass on weak augmentation\n",
        "            with torch.no_grad():\n",
        "                _, _, mu_cau_tea_unlab, _ = teacher_model(img_weak_unlab)\n",
        "\n",
        "            # Calculate Consistency Loss\n",
        "            loss_cr = consistency_loss_fn(mu_cau_stu_unlab, mu_cau_tea_unlab)\n",
        "\n",
        "        # ---------------------------------\n",
        "        # Total Loss and Backpropagation\n",
        "        # ---------------------------------\n",
        "        total_loss = loss_sup + config[\"consistency_lambda\"] * loss_cr\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update Teacher Model EMA weights\n",
        "        update_ema_variables(student_model, teacher_model, config[\"ema_decay\"], global_step)\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "        # Logging losses\n",
        "        epoch_loss += total_loss.item()\n",
        "        epoch_loss_cau += loss_cau.item() if config[\"labeled_bs\"] > 0 else 0\n",
        "        epoch_loss_con += loss_con.item() if config[\"labeled_bs\"] > 0 else 0\n",
        "        epoch_loss_bd += loss_bd.item() if config[\"labeled_bs\"] > 0 and sampled_eta_con is not None else 0\n",
        "        epoch_loss_cr += loss_cr.item() if num_unlabeled > 0 else 0\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f\"{total_loss.item():.4f}\",\n",
        "            'L_cau': f\"{loss_cau.item():.4f}\" if config[\"labeled_bs\"] > 0 else \"N/A\",\n",
        "            'L_con': f\"{loss_con.item():.4f}\" if config[\"labeled_bs\"] > 0 else \"N/A\",\n",
        "            'L_bd': f\"{loss_bd.item():.4f}\" if config[\"labeled_bs\"] > 0 and sampled_eta_con is not None else \"N/A\",\n",
        "            'L_cr': f\"{loss_cr.item():.4f}\" if num_unlabeled > 0 else \"N/A\"\n",
        "        })\n",
        "\n",
        "    # --- End of Epoch ---\n",
        "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "    avg_loss_cau = epoch_loss_cau / len(train_loader)\n",
        "    avg_loss_con = epoch_loss_con / len(train_loader)\n",
        "    avg_loss_bd = epoch_loss_bd / len(train_loader)\n",
        "    avg_loss_cr = epoch_loss_cr / len(train_loader)\n",
        "\n",
        "    # Log average epoch losses to TensorBoard\n",
        "    writer.add_scalar('Loss/Train_Total', avg_epoch_loss, epoch)\n",
        "    writer.add_scalar('Loss/Train_Causal', avg_loss_cau, epoch)\n",
        "    writer.add_scalar('Loss/Train_Confound', avg_loss_con, epoch)\n",
        "    writer.add_scalar('Loss/Train_Backdoor', avg_loss_bd, epoch)\n",
        "    writer.add_scalar('Loss/Train_Consistency', avg_loss_cr, epoch)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Average Loss: {avg_epoch_loss:.4f} [L_cau:{avg_loss_cau:.4f}, L_con:{avg_loss_con:.4f}, L_bd:{avg_loss_bd:.4f}, L_cr:{avg_loss_cr:.4f}]\")\n",
        "\n",
        "    # --- Validation ---\n",
        "    student_model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds_val = []\n",
        "    all_labels_val = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_val, labels_val in val_loader:\n",
        "            img_val, labels_val = img_val.to(config[\"device\"]), labels_val.to(config[\"device\"])\n",
        "            _, _, mu_cau_val, _ = student_model(img_val)\n",
        "            loss = causal_loss_fn(mu_cau_val, labels_val) # Use causal loss for validation\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(mu_cau_val, dim=1)\n",
        "            all_preds_val.extend(preds.cpu().numpy())\n",
        "            all_labels_val.extend(labels_val.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = accuracy_score(all_labels_val, all_preds_val)\n",
        "    val_precision = precision_score(all_labels_val, all_preds_val, average='binary', zero_division=0)\n",
        "    val_recall = recall_score(all_labels_val, all_preds_val, average='binary', zero_division=0)\n",
        "    val_f1 = f1_score(all_labels_val, all_preds_val, average='binary', zero_division=0)\n",
        "\n",
        "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
        "    writer.add_scalar('Metrics/Val_Accuracy', val_accuracy, epoch)\n",
        "    writer.add_scalar('Metrics/Val_Precision', val_precision, epoch)\n",
        "    writer.add_scalar('Metrics/Val_Recall', val_recall, epoch)\n",
        "    writer.add_scalar('Metrics/Val_F1', val_f1, epoch)\n",
        "\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch + 1) % 10 == 0 or epoch == config[\"epochs\"] - 1:\n",
        "        checkpoint_path = os.path.join(config[\"output_dir\"], \"checkpoints\", f\"cats_epoch_{epoch+1}.pth\")\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'student_state_dict': student_model.state_dict(),\n",
        "            'teacher_state_dict': teacher_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "writer.close()\n",
        "print(\"Training Finished.\")\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Evaluation on Test Set\n",
        "\n",
        "# %%\n",
        "print(\"Evaluating on Test Set...\")\n",
        "student_model.eval() # Use the final student model for testing\n",
        "test_loss = 0.0\n",
        "all_preds_test = []\n",
        "all_labels_test = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img_test, labels_test in tqdm(test_loader, desc=\"Testing\"):\n",
        "        img_test, labels_test = img_test.to(config[\"device\"]), labels_test.to(config[\"device\"])\n",
        "\n",
        "        # Forward pass through the student model's causal path\n",
        "        _, _, mu_cau_test, _ = student_model(img_test)\n",
        "\n",
        "        loss = causal_loss_fn(mu_cau_test, labels_test)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(mu_cau_test, dim=1)\n",
        "        all_preds_test.extend(preds.cpu().numpy())\n",
        "        all_labels_test.extend(labels_test.cpu().numpy())\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "test_accuracy = accuracy_score(all_labels_test, all_preds_test)\n",
        "test_precision = precision_score(all_labels_test, all_preds_test, average='binary', zero_division=0)\n",
        "test_recall = recall_score(all_labels_test, all_preds_test, average='binary', zero_division=0)\n",
        "test_f1 = f1_score(all_labels_test, all_preds_test, average='binary', zero_division=0)\n",
        "\n",
        "print(\"\\n--- Test Set Results ---\")\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Optional: Save Final Model\n",
        "\n",
        "# %%\n",
        "final_model_path = os.path.join(config[\"output_dir\"], \"cats_final_student_model.pth\")\n",
        "torch.save(student_model.state_dict(), final_model_path)\n",
        "print(f\"Final student model saved to {final_model_path}\")"
      ]
    }
  ]
}